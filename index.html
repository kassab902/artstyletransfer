<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <link rel="stylesheet" href="/artstyletransfer/assets/css/style.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="shortcut icon" type="image/x-icon" href="/artstyletransfer/thumbnail.png">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
<!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet" /> -->
<!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" /> -->

<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
<!-- <script src="https://unpkg.com/@popperjs/core@2" ></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.9.2/umd/popper.min.js" integrity="sha512-2rNj2KJ+D8s1ceNasTIex6z4HWyOnEYLVC3FigGOmyQCZc2eBXKgOxQmo3oKLHyfcj53uz4QMsRCWNbLd32Q1g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jimp/0.16.1/jimp.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.13.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite/dist/tf-tflite.min.js"></script> -->

<link href="/artstyletransfer/assets/css/stylizer.css" rel="stylesheet">
<!-- <script src="/artstyletransfer/assets/js/stylizer.js" ></script> -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!-- end custom head snippets -->


  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Neural image stylization with Attention</a></h1>

        

        <p>Image stylization made cheap!</p>

        

        

        
      </header>

      <section>

      <p>Written by: <a href="https://github.com/kassab902">@akassab</a> and <a href="https://copilot.github.com/">@copilot</a>. Code available <a href="https://github.com/kassab902/artstyletransfer">here</a>.</p>

<p>So, <a href="https://arxiv.org/abs/1706.03762">attention</a> is dominating the AI field now. Not only for NLP, but also for pretty much <a href="https://arxiv.org/abs/2103.05247">any <em>computable</em> task out there</a>. So, why not use this approach for image stylization as well? After all, if a human were to stylize an image by hand, it’s very much expected that they would pay attention to different parts of the style image when deciding how to stylize, say the Sun on the content image.</p>

<p>So, that’s what <a href="https://arxiv.org/abs/1812.02342v5">these guys did</a>. They created a network that takes an <em>arbitrary</em> content and style image pair, and then tries to find the best way to blend the two – using attention.</p>

<p>Let’s see how it works. Embedded below is a TensorflowJS application that will use the network described above two artistically render the content image. Click <code class="language-plaintext highlighter-rouge">Stylize!</code> to begin the process. Note that the warmup will take a few seconds depending on your hardware since it’s using <a href="https://arxiv.org/abs/1409.1556">chunky VGG-19</a> for inference).</p>

<section>
<div class="container" style="width: 500px">
  <div class="row justify-content-sm-center">
    <div class="col-sm-6 imgUp">
      <div class="caption">
        <p>Content Image</p>
      </div>
      <img id="ContentImage" class="imagePreview img-thumbnail" src="/artstyletransfer/assets/img/boston.jpg" />
      <label class="btn btn-secondary btn-lg thumbnail-btn">
        Choose Content<input type="file" class="uploadFile img" id="ContentInput" style="width: 0px;height: 0px;overflow: hidden;" />
      </label>
    </div><!-- col-2 -->
    <!-- <i class="fa fa-plus imgAdd"></i> -->

    <div class="col-sm-6 imgUp">
      <div class="caption">
        <p>Style Image</p>
      </div>
      <!-- <div class="imagePreview img-thumbnail"></div> -->
      <img id="StyleImage" class="imagePreview img-thumbnail" src="/artstyletransfer/assets/img/la_muse.jpg" />

      <label class="btn btn-secondary  btn-lg thumbnail-btn">
        Choose Style<input type="file" class="uploadFile img" id="StyleInput" style="width: 0px;height: 0px;overflow: hidden;" />
      </label>
    </div><!-- col-2 -->
    <!-- <i class="fa fa-plus imgAdd"></i> -->
    <div class="col-sm-6 imgUp">
      <label for="max_resolution" class="form-label">Quality</label>
      <input type="range" class="form-range" min="126" max="512" value="256" step="126" id="max_resolution" />
   </div>
    <!-- The output -->
    <!-- Image zoom from: https://alt-web.com/TUTORIALS/?id=bootstrap_simple_image_zoom -->
    <div class="row justify-content-sm-center">
      <div class="col-sm-offset-3 col-sm-6">
        <!-- source: https://alt-web.com/TUTORIALS/?id=bootstrap_simple_image_zoom -->
        <!-- <div class="imagePreview thumbnail zoom result-img"></div>     -->
        <!-- <canvas id="Result" class="imagePreview img-thumbnail zoom"></canvas> -->
        <canvas id="Result" class="imagePreview img-thumbnail zoom" src="http://cliquecities.com/assets/no-image-e3699ae23f866f6cbdf8ba2443ee5c4e.jpg"></canvas>

        <label id="Stylize" class="btn btn-primary btn-lg thumbnail-btn" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Might take 20-30s on the first try! (depends on the hardware)">
          <!-- <span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span>
        Stylizing... -->
          Stylize!
        </label>
        <!-- There likely is a problem with bootstrap 3  spinners. 
        Maybe not implemented until 4? -->

        <!-- <button class="btn btn-primary" type="button" id="Stylize" disabled>
        <span class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span>
        Loading...
        Stylize<input type="file" class="uploadFile img" value="Upload Photo"
          style="width: 0px;height: 0px;overflow: hidden;">
      </button> -->
      </div>
    </div>
    <!-- end output -->
  </div><!-- container -->
</div>
</section>

<h2 id="so-how-does-it-work">So how does it work?</h2>

<p>Well, the real question is how’s it work <em>in</em> TensorflowJS, given that the original source came from <a href="https://github.com/GlebSBrykin/SANET">PyTorch repo</a>?</p>

<p>We (the copilot &amp; me) will also explain the model architecture in detail.</p>

<h3 id="the-model-architecture">The model architecture</h3>

<p>An image is worth a <del>1000 words</del> <a href="https://arxiv.org/abs/2010.11929">16x16 words</a>. So, let’s see the image of the architecture.</p>

<p><img src="/artstyletransfer/assets/img/model-arch.png" alt="Model architecture" /></p>

<p>That’s one messy-looking model, but fear not. The copilot and me are going to explain it all. Let’s read the image image from left to right. The two inputs are the content (\(I_c\)) and style (\(I_s\))images. The content image is the image that we want to stylize. The style image is the image that we want to use as a style reference.</p>

<p>These two inputs are first fed into the frozen <code class="language-plaintext highlighter-rouge">Encoder VGG</code> network, but only up to the <code class="language-plaintext highlighter-rouge">block4_conv1</code> and <code class="language-plaintext highlighter-rouge">block5_conv1</code> layers. This is because we want to use the encoder to extract features of different complexity of from the content and style images. We’ll use these features to create a new image that is a blend of the content and style images.</p>

<p>The four short horizontal lines, \(F_c^{r_{41}}\), \(F_c^{r_{51}}\) and \(F_s^{r_{41}}\), \(F_s^{r_{51}}\) are the content and style representations of the content and style images.</p>

<p>Now, we apply main ingredient to the mix, the attention! The two pairs of similar sized \(r_{41}\) and \(r_{51}\) images are pushed through the following module:</p>

<p><img src="/artstyletransfer/assets/img/attn.png" alt="Attention module" /></p>

<p>The purpose of this segment is to embed local style patterns Fs within content feature maps Fc, by using learned \(1\times1\) convolutions and attention (shown as \(3\times3\) black-white matrix). This block implements the following mathematical relationship between inputs \(F_c\) and \(F_s\) to outputs \(F_cs\):</p>

<!-- ![Equation](/artstyletransfer/assets/img/eqn.png) -->

\[F^i_{cs} = \frac{1}{C(F)} \sum_{\forall j} \exp(f(\bar{F^i_c})^T g(F^j_s) )h(F^j_s)\]

<p>This might look familiar to someone who knows what the \(softmax\) function is. In fact, this <em>is</em> a softmax function multiplied with the \(h(Fs)\). Conceptually, the purpose of the above equation is to create a way of re-constructing the content using the style feature map. In particular, the equation first calculates attention between the two \(g(\bar{F_s}) = W_g\bar{F_s}\) and \(f(\bar{F_c}) = W_f\bar{F_c}\) feature maps. This is done in the \(F^i_{cs} = \frac{1}{C(F)} \sum_{\forall j} \exp(f(\bar{F^i_c})^T g(F^j_s) )\) part, where \(C(F) = \sum_{\forall{j}}\exp(f(\bar{F^i_c})^T g(F^j_s) )\) is the normalization factor (makes softmax results fall between 0 and one, like probabilities). The softmax operation yields a \(32^2\times 32^2\) matrix of probabilities. This matrix is then multiplied with This matrix is then multiplied with \(h(F^j_s)\) producing a weighted average of the pixels in the style feature map. Thus, the SANet block learns such a relationship between the content and style (using \(W_f\) and \(W_g\) weights) that can be used to recombine style feature maps into the final image. This is the core idea of SANet that allows it to create much more detailed stylized outputs.</p>

<h3 id="implementing-sanet">Implementing SANet</h3>

<h4 id="first-deployment-pains">First Deployment Pains</h4>
<p>SANet authors provide the implementation of their paper in the following <a href="https://github.com/GlebSBrykin/SANET">github repository</a>. Initial challenges ranged from setting up the environment correctly in order to run inference, to fixing various bugs caused by outdated dependencies. The project is written in PyTorch. However, right from the start of this project, the end-goal was to create a user-friendly web-application. We began our implementation of this webapp by deploying the code directly on DigitalOcean. This deployment, however, would prove to be quite problematic, since our web app was extremely slow to load and prone to crashes due to the limited RAM the server had.</p>

<p>Soon it became apparent that deploying SANet meant renting a costly, GPU-backed server.  Otherwise it would likely get overwhelmed by computational requirements. This, coupled with the internet latency issues and likely a private nature of the images uploaded there, prompted us to seek other solutions.</p>

<p>As of Jan 2022, TensorflowJS is likely the only javascript based ML framework that is both fast and easy to use. The javascript-based nature of TFJS means that any model that is implemented within TFJS can run without a server. A server provides around 80MB of network weights just once, after which the client runs the network on their own device. This eliminates the latency, privacy and cost issues associated with the original PyTorch implementation. However, we will have to transfer the PyTorch network weights to TensorflowJS to do that.</p>

<h4 id="transferring-the-network-to-tensorflowjs">Transferring the Network to TensorflowJS</h4>

<p>We quickly learned that there is no direct path to converting the PyTorch model to Tensorflow, let alone to TensorflowJS. There is, however, an intermediate framework, called Open neural network exchange (ONNX), which aims to act as a glue between various ML frameworks, such as MXNet, Jax, Pytorch and Tensorflow. It is possible to export specially constructed PyTorch models into ONNX format. The original implementation does not satisfy the ONNX conversion rules. To fix this, we first re-implemented the SANet within the PyTorch framework, namely, by writing a new implementation of the SANet block which avoids the unsupported dynamic reshape operation by a supported <code class="language-plaintext highlighter-rouge">flatten(A,2)</code> operation. Additionally, in order to support varying image input sizes, we disable constant folding and specify dynamic axes in onnx.export function (dynamic axes, like height and width are not fixed). After these modifications and fixing other minor issues, we export the PyTorch model into three separate parts: <code class="language-plaintext highlighter-rouge">encoder.onnx</code>, <code class="language-plaintext highlighter-rouge">decoder.onnx</code> and <code class="language-plaintext highlighter-rouge">transform.onnx</code>, corresponding to the same three parts in the architecture. This split into three parts is necessary since the full SANet algorithm requires a for-loop, which is not possible to export within ONNX.</p>

<p>Next, we used the <a href="https://github.com/onnx/onnx-tensorflow">onnx-tensorflow</a> library to transform the three ONNX model files into three Tensorflow saved-models. We then loaded in the three Tensorflow saved-models separately and created a new <code class="language-plaintext highlighter-rouge">tf.Module</code> subclass, called <code class="language-plaintext highlighter-rouge">TFStyle</code>. This class would perform all the necessary calculations on the two input images, including the required for-loop operation. We then validated that the <code class="language-plaintext highlighter-rouge">TFStyle</code> class outputs match those of the PyTorch implementation to a floating point precision. At this point, we only need to set-up the <code class="language-plaintext highlighter-rouge">Tensorflow_Serving</code> signature for this model in order to be able to safely export it to a TensorflowJS environment. We designed the serving signature to:</p>

<ul>
  <li>Accept any two arbitrarily-shaped input images</li>
  <li>Accept two additional parameters (num_iterations, max_resolution)</li>
  <li>Rescale and normalize the images (using max_resolution)</li>
  <li>Perform stylization (including the num_iterations for style strength)</li>
  <li>Reshape the output to the shape of the original content image</li>
  <li>Return results</li>
</ul>

<p>Note that, as it currently stands, the <code class="language-plaintext highlighter-rouge">num_iterations</code> argument is fixed to be <code class="language-plaintext highlighter-rouge">2</code> in the website. This means that the content image is stylized once and then the result is stylized once more with the same style. Higher <code class="language-plaintext highlighter-rouge">num_iterations</code> will strengthen the style influence at the cost of more computation. <del>We also lack a quality control lever</del> We have added a quality control lever, but some bugs still persist (images are generated with the same shape, despite the quality changes).</p>

<p>Finally, the TFStyle model, along with the Serving signature is exported to a JSON format using the TensorflowJS converter tool. This generates an 80MB model folder that can be executed using the TensorflowJS framework, within a web app.</p>

<h4 id="implementing-the-website">Implementing the website</h4>

<p>The website was implemented in two stages. First, a draft implementation was made, in pure HTML and Javascript, in order to validate that the exported TFJS model was not corrupted.</p>

<p>During development of the first iteration, several problems were encountered and fixed. These include problems with javascript package versions, image data loading from the site and various TensorflowJS issues, mainly due to lack of knowledge of Javascript.</p>

<p>Finally, after troubleshooting the errors in the first iteration of the website, we implemented an interactive blog-like website, shown in Figure 5. (right). This website utilizes a popular and simple blog-creation tool, Jekyll. Jekyll works by converting a plain markup language into a blog-like website. This conversion happens only once and the resulting HTML and CSS files are saved on disk. However, in order to make a website available to the general public, we needed to use a service called GitHub-Pages, which allows for free placement of a website under a unique domain name. We used GitHub-Pages, alongside with GitHub Actions (a continuous Integration service) in order to automatically recompile and update the website when GitHub detects any change with the repository. All this happens automatically and requires no human supervision. Lastly, we added a blog around the interactive stylization app that describes the inner workings and details of the SANet to serve as a future portfolio/showcase website.</p>

<h3 id="limitations">Limitations</h3>

<p>With the current implementation of the style transfer web application, there are several outstanding problems. One of the most noticeable ones is the loading delay. The first run of the model can take up to 30s, while the subsequent calls only need up to a second (depending on the image resolution). We think this could be fixed by using knowledge distillation on encoder and decoder VGG networks, as these take up more than 95% of the network weights. Quantization is also a promising candidate and can be applied relatively easily using the TFLite library.  Another solution would be to forego the VGG architecture altogether and instead use MobileNet architecture, as these are much more performant and optimized for mobile use. This would likely require re-designing some key components of the SANet.</p>

<p>Finally, the num_iterations and max_resolution variables are not available to the users on the published blog. This could be easily fixed by adding the appropriate input sliders to the form.</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We thank Dr. Jacob Levman for offering guidance and insightful comments during the development of this website. We also thank Jessica Levman for providing us with sample art for testing our early iterations of the style transfer application.</p>

<h3 id="conclusion-and-future-work">Conclusion and Future Work</h3>

<p>In this report we discussed our achievements while working on the subject of neural style transfer. In doing so, we learned the basics of PyTorch and ONNX, mastered Tensorflow and TensorflowJS, and worked with Jekyll and various development tools, such as Github actions and Github pages.</p>

<p>In the future, we would like to make the SANet faster, smaller and more accessible. We would also like to implement a video stylization application. Last but not least, this application could be used as a basis for an iOS or an Android application.</p>



      </section>
      <footer>
        
        <p><small>Hosted on <a href="https://github.com/kassab902/artstyletransfer">GitHub Pages</small></p>
      </footer></a> 
    </div>
    <script src="/artstyletransfer/assets/js/stylizer.js"></script>
  </body>
</html>
